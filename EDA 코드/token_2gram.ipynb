{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#비윤리 텍스트 파일 열기\n",
    "f = open('/Users/idayeon/Desktop/인공지능학부연구생/EDA/textData.txt','r',encoding = 'UTF-16')\n",
    "lines = f.readlines()\n",
    "del lines[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#텍스트 그대로 토큰화 한 버전\n",
    "from konlpy.tag import Mecab\n",
    "\n",
    "tokenizer = Mecab()\n",
    "tokenized = []\n",
    "for sentence in lines:\n",
    "    tmp = []\n",
    "    tmp = tokenizer.morphs(sentence)\n",
    "    for token in tmp:\n",
    "        tokenized.append(token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#모든 토큰화된 텍스트마다(불용어 삭제 전, 후 토큰화된 텍스트) 이 두 함수 적용 시키자.\n",
    "#2그램 분석 함수 생성\n",
    "def two_gram(text):\n",
    "    global gram_words\n",
    "    gram_words = {}\n",
    "    import operator\n",
    "    for i in range(len(text)-1) :\n",
    "        g_word = []\n",
    "        g_word.append(text[i])\n",
    "        g_word.append(text[i+1])\n",
    "        g_word = tuple(g_word)\n",
    "        if g_word in gram_words :\n",
    "            gram_words[g_word] += 1\n",
    "        else :\n",
    "            gram_words[g_word] = 1\n",
    "    sorted_dic = sorted(gram_words.items(),key = operator.itemgetter(1), reverse = True)\n",
    "    return sorted_dic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "origin = two_gram(tokenized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(origin, columns = ['2_gram','frequency'])\n",
    "df.to_csv('origin_token_2gram.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#불용어 단어들 리스트 생성\n",
    "st_f = open('/Users/idayeon/Desktop/인공지능학부연구생/EDA/stopwords_list.txt', encoding = 'UTF-8')\n",
    "stopwords_list = st_f.readlines()\n",
    "strip_list = []\n",
    "for i in stopwords_list:\n",
    "    i = i.strip()\n",
    "    strip_list.append(i)\n",
    "stopwords=['뭐','으면','을','의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다','이', '있', '하', '것', '들', '그', '되', '수', '이', '보', '않', '없', '나', '사람', '주', '아니', '등', '같', '우리', '때', '년', '가', '한', '지', '대하', '오', '말', '일', '그렇', '위하']\n",
    "for i in stopwords :\n",
    "    if i not in strip_list:\n",
    "        strip_list.append(i)\n",
    "#불용어 삭제 후 토큰화한 버전\n",
    "from konlpy.tag import Mecab\n",
    "tokenizer = Mecab()\n",
    "res = []\n",
    "for sentence in lines:\n",
    "    tmp = []\n",
    "    tmp = tokenizer.morphs(sentence)\n",
    "  \n",
    "    tokenized = []\n",
    "    for token in tmp:\n",
    "        if not token in strip_list:\n",
    "            tokenized.append(token)\n",
    "    for j in tokenized :\n",
    "        res.append(j)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#불용어 삭제 후 토큰화 한 데이터 프레임 파일로 저장\n",
    "import pandas as pd\n",
    "data = two_gram(res)\n",
    "df = pd.DataFrame(data, columns = ['2_gram','frequency'])\n",
    "df.to_csv('no_stopwords_2gram.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['아무', '시간', '개념', '사람', '개념', '사람', '인방', '남자', '게', '사회']\n"
     ]
    }
   ],
   "source": [
    "#명사들만 추출 \n",
    "from konlpy.tag import Mecab\n",
    "mecab = Mecab()\n",
    "\n",
    "nouns_list = []\n",
    "for i in lines :\n",
    "    noun_s = mecab.nouns(i)\n",
    "    for j in noun_s:\n",
    "        nouns_list.append(j)\n",
    "print(nouns_list[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#명사로 2gram 분석\n",
    "import pandas as pd\n",
    "data = two_gram(nouns_list)\n",
    "df = pd.DataFrame(data, columns = ['2_gram','frequency'])\n",
    "df.to_csv('nouns_2gram.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#텍스트 파일을 string 타입을 변환시킨 다음, 띄어쓰기 별로 구분하여 토큰화\n",
    "file = open('/Users/idayeon/Desktop/인공지능학부연구생/EDA/textData.txt','r',encoding = 'UTF-16')\n",
    "contents = \"\"\n",
    "while True :\n",
    "    line = file.readline()\n",
    "    if(not line) :\n",
    "        break\n",
    "    contents += line\n",
    "\n",
    "words = contents.split()       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#띄어쓰기 별 토크화 2그램 데이터 프레임 파일로 저장\n",
    "import pandas as pd\n",
    "data = two_gram(words)\n",
    "split_2gram_df = pd.DataFrame(data, columns = ['2_gram','frequency'])\n",
    "split_2gram_df.to_csv(\"split_word_2gram.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (main, Aug 25 2022, 18:24:45) \n[Clang 12.0.0 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b733b73d7d6d76c6a583195fffde7d15920d8bfb4832083bd204139a0e6d80a1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
